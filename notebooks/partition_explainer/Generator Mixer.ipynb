{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "Python 3.8.5 64-bit",
   "display_name": "Python 3.8.5 64-bit",
   "metadata": {
    "interpreter": {
     "hash": "055a9eea696f8feaedde792ab6a62d395f82bb567379528fa7782c67c6efb646"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import copy\n",
    "import shutil\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mlp\n",
    "%matplotlib inline\n",
    "from transformers import AutoTokenizer, AutoModelWithLMHead, AutoModelForSeq2SeqLM, BartForConditionalGeneration,BartTokenizer\n",
    "import shap\n",
    "import scipy as sp\n",
    "import nlp\n",
    "import torch\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('max_colwidth', None)\n",
    "pd.set_option(\"max_rows\", None)\n",
    "from shap.utils import assert_import, record_import_error, safe_isinstance, make_masks, OpChain, MaskedModel\n",
    "from shap import maskers, links\n",
    "from transformers.file_utils import ModelOutput\n",
    "#from shap.utils import cal_conditional_logits\n",
    "import sys\n",
    "np.set_printoptions(formatter={'float': lambda x: \"{0:0.5f}\".format(x)})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = BartTokenizer.from_pretrained(\"sshleifer/distilbart-xsum-12-6\")\n",
    "model =  BartForConditionalGeneration.from_pretrained(\"sshleifer/distilbart-xsum-12-6\").cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "sent1=\"Hi my dog is cute\"\n",
    "sent2=\"Hello my puppy is cute\"\n",
    "#input_ids = torch.tensor(tokenizer.encode(sent1)).unsqueeze(0).cuda()\n",
    "#output_ids = torch.tensor(tokenizer.encode(sent2)).unsqueeze(0).cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "tensor([[    0, 30086,   127,  2335,    16, 11962,     2]], device='cuda:0')"
      ]
     },
     "metadata": {},
     "execution_count": 4
    }
   ],
   "source": [
    "input_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "tensor([[    0, 31414,   127, 20830,    16, 11962,     2]], device='cuda:0')"
      ]
     },
     "metadata": {},
     "execution_count": 5
    }
   ],
   "source": [
    "output_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "#model.train()\n",
    "with torch.no_grad():\n",
    "    outputs = model(input_ids=input_ids, decoder_input_ids=output_ids,labels=output_ids,return_dict=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "torch.Size([1, 7, 50264])"
      ]
     },
     "metadata": {},
     "execution_count": 7
    }
   ],
   "source": [
    "outputs.logits.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "lm_tokenizer = AutoTokenizer.from_pretrained(\"distilgpt2\")\n",
    "lm_model = AutoModelWithLMHead.from_pretrained(\"distilgpt2\").cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "lm_input_ids = torch.tensor(lm_tokenizer.encode(sent1)).unsqueeze(0).cuda()\n",
    "lm_output_ids = torch.tensor(lm_tokenizer.encode(sent2)).unsqueeze(0).cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "tensor([[17250,   616,  3290,   318, 13779]], device='cuda:0')"
      ]
     },
     "metadata": {},
     "execution_count": 10
    }
   ],
   "source": [
    "lm_input_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "tensor([[15496,   616, 26188,   318, 13779]], device='cuda:0')"
      ]
     },
     "metadata": {},
     "execution_count": 11
    }
   ],
   "source": [
    "lm_output_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "concat_inputs = torch.cat((lm_input_ids,lm_output_ids),dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "tensor([[17250,   616,  3290,   318, 13779, 15496,   616, 26188,   318, 13779]],\n",
       "       device='cuda:0')"
      ]
     },
     "metadata": {},
     "execution_count": 13
    }
   ],
   "source": [
    "concat_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "past=None\n",
    "with torch.no_grad():\n",
    "    output = lm_model(lm_input_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "tensor([15496], device='cuda:0')"
      ]
     },
     "metadata": {},
     "execution_count": 15
    }
   ],
   "source": [
    "lm_output_ids[0,0].unsqueeze(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "tensor([[-48.9168, -52.4876, -55.5191,  ..., -65.9269, -62.6038, -51.5396]])\ntensor([-42.9091, -48.0814, -51.4108,  ..., -57.4924, -56.4301, -48.2307])\ntensor([-39.2923, -41.9795, -44.2819,  ..., -49.5130, -47.5725, -42.0874])\ntensor([-43.7650, -47.3087, -49.6883,  ..., -59.5156, -54.8545, -46.3371])\ntensor([-47.3736, -50.3058, -51.4557,  ..., -61.9204, -56.3025, -50.7423])\n"
     ]
    }
   ],
   "source": [
    "past=None\n",
    "for i in range(0,lm_output_ids.shape[1]):\n",
    "    if i==0:\n",
    "        with torch.no_grad():\n",
    "            output, past = lm_model(lm_input_ids, past=past)           \n",
    "    else:\n",
    "        with torch.no_grad():\n",
    "            output, past = lm_model(lm_output_ids[0,i-1].unsqueeze(0), past=past)\n",
    "    next_token_logits = output[..., -1, :].detach().cpu()\n",
    "    print(next_token_logits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "lm_model.eval()\n",
    "past=None\n",
    "with torch.no_grad():\n",
    "    outputs = lm_model(concat_inputs,return_dict=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "tensor([[[-32.9849, -31.3989, -33.2205,  ..., -44.7180, -44.6273, -33.4917],\n",
       "         [-50.4431, -50.5061, -51.8230,  ..., -53.8949, -55.2346, -51.0417],\n",
       "         [-56.9040, -59.4501, -61.7221,  ..., -70.2842, -65.0359, -59.3530],\n",
       "         ...,\n",
       "         [-43.7650, -47.3087, -49.6883,  ..., -59.5156, -54.8545, -46.3371],\n",
       "         [-47.3736, -50.3058, -51.4557,  ..., -61.9204, -56.3025, -50.7422],\n",
       "         [-38.3973, -41.4092, -42.6722,  ..., -54.3608, -51.5736, -39.0006]]],\n",
       "       device='cuda:0')"
      ]
     },
     "metadata": {},
     "execution_count": 18
    }
   ],
   "source": [
    "outputs.logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "tensor([[-48.9168, -52.4876, -55.5191,  ..., -65.9269, -62.6038, -51.5396]],\n       device='cuda:0')\ntensor([[-42.9091, -48.0814, -51.4108,  ..., -57.4924, -56.4301, -48.2307]],\n       device='cuda:0')\ntensor([[-39.2923, -41.9795, -44.2819,  ..., -49.5130, -47.5725, -42.0874]],\n       device='cuda:0')\ntensor([[-43.7650, -47.3087, -49.6883,  ..., -59.5156, -54.8545, -46.3371]],\n       device='cuda:0')\ntensor([[-47.3736, -50.3058, -51.4557,  ..., -61.9204, -56.3025, -50.7422]],\n       device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "for i in range(lm_input_ids.shape[1]-1,lm_input_ids.shape[1]+lm_output_ids.shape[1]-1):\n",
    "    print(outputs.logits[:,i,:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "tensor([[-48.9168, -52.4876, -55.5191,  ..., -65.9269, -62.6038, -51.5396]],\n       device='cuda:0')\ntensor([[-42.9091, -48.0814, -51.4108,  ..., -57.4924, -56.4301, -48.2307]],\n       device='cuda:0')\ntensor([[-39.2923, -41.9795, -44.2819,  ..., -49.5130, -47.5725, -42.0874]],\n       device='cuda:0')\ntensor([[-43.7650, -47.3087, -49.6883,  ..., -59.5156, -54.8545, -46.3371]],\n       device='cuda:0')\ntensor([[-47.3736, -50.3058, -51.4557,  ..., -61.9204, -56.3025, -50.7422]],\n       device='cuda:0')\ntensor([[-38.3973, -41.4092, -42.6722,  ..., -54.3608, -51.5736, -39.0006]],\n       device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "for i in range(lm_input_ids.shape[1]-1,lm_input_ids.shape[1]+lm_output_ids.shape[1]):\n",
    "    print(outputs.logits[:,i,:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "metadata": {},
     "execution_count": 21
    }
   ],
   "source": [
    "lm_input_ids.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "torch.Size([1, 5])"
      ]
     },
     "metadata": {},
     "execution_count": 22
    }
   ],
   "source": [
    "lm_output_ids.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GenerateLogits:\n",
    "    def __init__(self,model=\"distilgpt2\",tokenizer=None):\n",
    "        if isinstance(model,str):\n",
    "            # load model and tokenizer from transformers library\n",
    "            self.model = AutoModelWithLMHead.from_pretrained(model)\n",
    "            self.tokenizer = AutoTokenizer.from_pretrained(model)\n",
    "        else:\n",
    "            self.model = model\n",
    "            self.tokenizer = tokenizer\n",
    "        self.device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "        self.model = self.model.to(self.device)\n",
    "        null_tokens = self.tokenizer.encode(\"\")\n",
    "\n",
    "        if len(null_tokens) == 0:\n",
    "            self.keep_prefix = 0\n",
    "            self.keep_suffix = 0\n",
    "        elif len(null_tokens) == 1:\n",
    "            null_token = null_tokens[0]\n",
    "            assert (('eos_token' in self.tokenizer.special_tokens_map) or ('bos_token' in self.tokenizer.special_tokens_map)), \"No eos token or bos token found in tokenizer!\"\n",
    "            if ('eos_token' in self.tokenizer.special_tokens_map) and (self.tokenizer.decode(null_token) == self.tokenizer.special_tokens_map['eos_token']):\n",
    "                self.keep_prefix = 0\n",
    "                self.keep_suffix = 1\n",
    "            elif ('bos_token' in self.tokenizer.special_tokens_map) and (self.tokenizer.decode(null_token) == self.tokenizer.special_tokens_map['bos_token']):\n",
    "                self.keep_prefix = 1\n",
    "                self.keep_suffix = 0\n",
    "        else:\n",
    "            assert len(null_tokens) % 2 == 0, \"An odd number of boundary tokens are added to the null string!\"\n",
    "            self.keep_prefix = len(null_tokens) // 2\n",
    "            self.keep_suffix = len(null_tokens) // 2\n",
    "\n",
    "    def get_teacher_forced_logits(self,source_sentence_ids,target_sentence_ids):\n",
    "        self.model.eval()\n",
    "        if self.model.config.is_encoder_decoder:\n",
    "            # assigning decoder start token id \n",
    "            decoder_start_token_id = None\n",
    "\n",
    "            if hasattr(self.model.config, \"decoder_start_token_id\") and self.model.config.decoder_start_token_id is not None:\n",
    "                decoder_start_token_id = self.model.config.decoder_start_token_id\n",
    "            elif hasattr(self.model.config, \"bos_token_id\") and self.model.config.bos_token_id is not None:\n",
    "                decoder_start_token_id = self.model.config.bos_token_id\n",
    "            elif (hasattr(self.model.config, \"decoder\") and hasattr(self.model.config.decoder, \"bos_token_id\") and self.model.config.decoder.bos_token_id is not None):\n",
    "                decoder_start_token_id = self.model.config.decoder.bos_token_id\n",
    "            else:\n",
    "                raise ValueError(\n",
    "                    \"No decoder_start_token_id or bos_token_id defined in config for encoder-decoder generation\"\n",
    "                )\n",
    "\n",
    "            target_sentence_start_id = torch.tensor([[decoder_start_token_id]]).to(self.device)\n",
    "            target_sentence_ids = torch.cat((target_sentence_start_id,target_sentence_ids),dim=-1)\n",
    "            with torch.no_grad():\n",
    "                outputs = self.model(input_ids=source_sentence_ids, decoder_input_ids=target_sentence_ids, labels=target_sentence_ids, return_dict=True)\n",
    "            logits=outputs.logits.detach().cpu().numpy()\n",
    "        else:\n",
    "            combined_sentence_ids = torch.cat((source_sentence_ids,target_sentence_ids),dim=-1)\n",
    "            with torch.no_grad():\n",
    "                outputs = self.model(input_ids=combined_sentence_ids, return_dict=True)\n",
    "            logits=outputs.logits.detach().cpu().numpy()[:,source_sentence_ids.shape[1]-1:,:]\n",
    "        del outputs\n",
    "        return logits\n",
    "\n",
    "    def get_sentence_ids(self, source_sentence,target_sentence):\n",
    "        source_sentence_ids = torch.tensor([self.tokenizer.encode(source_sentence)])\n",
    "        if self.keep_suffix > 0:\n",
    "            target_sentence_ids = torch.tensor([self.tokenizer.encode(target_sentence)])[:,self.keep_prefix:-self.keep_suffix]\n",
    "        else:\n",
    "            target_sentence_ids = torch.tensor([self.tokenizer.encode(target_sentence)])[:,self.keep_prefix:]\n",
    "        return source_sentence_ids.to(self.device), target_sentence_ids.to(self.device)\n",
    "\n",
    "    def get_output_names(self, sentence):\n",
    "        return self.tokenizer.tokenize(sentence)\n",
    "\n",
    "    def generate(self,source_sentence, target_sentence):\n",
    "        source_sentence_ids, target_sentence_ids = self.get_sentence_ids(source_sentence,target_sentence)\n",
    "        logits = self.get_teacher_forced_logits(source_sentence_ids,target_sentence_ids)\n",
    "        conditional_logits = []\n",
    "        for i in range(0,logits.shape[1]-1):\n",
    "            probs = (np.exp(logits[0][i]).T / np.exp(logits[0][i]).sum(-1)).T\n",
    "            logit_dist = sp.special.logit(probs)\n",
    "            conditional_logits.append(logit_dist[target_sentence_ids[0,i].item()])\n",
    "        return np.array(conditional_logits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "logit_model = GenerateLogits()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[-12.81402 -3.68543 -4.37839 -0.50151 -0.62419]\n"
     ]
    }
   ],
   "source": [
    "print(logit_model.generate(source_sentence=sent1,target_sentence=sent2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "logit_model = GenerateLogits(model = \"sshleifer/distilbart-xsum-12-6\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[-5.35224 -4.15271 -6.16725 -1.70498 0.41188]\n"
     ]
    }
   ],
   "source": [
    "print(logit_model.generate(source_sentence=sent1,target_sentence=sent2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "['Its', 'Ġa', 'Ġsun', 'n', 'ny', 'Ġday']"
      ]
     },
     "metadata": {},
     "execution_count": 15
    }
   ],
   "source": [
    "logit_model.tokenizer.tokenize(\"Its a sunnny day\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "[0, 30872, 10, 3778, 282, 2855, 183, 2]"
      ]
     },
     "metadata": {},
     "execution_count": 16
    }
   ],
   "source": [
    "logit_model.tokenizer.encode(\"Its a sunnny day\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}